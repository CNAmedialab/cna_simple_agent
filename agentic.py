
from agents import Agent, Runner, FileSearchTool, ModelSettings
from datetime import datetime, timedelta
from token_manager import get_token_manager
from typing import Dict
from pydantic import BaseModel
from func import *
from prompts import *
import asyncio
import time
import os
from dotenv import load_dotenv

# Get today's date
today_date = datetime.now().strftime("%Y-%m-%d")

# Initialize token manager
token_manager = get_token_manager()

# Token-aware agent runner
async def run_agent_with_token_tracking(agent: Agent, 
                                      input_data: str, 
                                      context=None,
                                      max_retries: int = 2) -> object:
    """
    Run agent with token usage tracking and overflow protection
    
    Args:
        agent: The agent to run
        input_data: Input data for the agent
        context: Optional context
        max_retries: Maximum retries if token limit exceeded
        
    Returns:
        Agent result with token tracking
    """
    for attempt in range(max_retries + 1):
        try:
            # Estimate input tokens
            estimated_input_tokens = token_manager.estimate_tokens_for_agent_input(
                instructions=agent.instructions,
                user_input=input_data,
                model=agent.model
            )
            
            # Check token limits
            is_within_limit, available_tokens, model_limit = token_manager.check_token_limit(
                estimated_input_tokens, agent.model
            )
            
            if not is_within_limit:
                print(f"âš ï¸  Token é™åˆ¶è­¦å‘Š - {agent.name} ({agent.model})")
                print(f"   é ä¼° input tokens: {estimated_input_tokens:,}")
                print(f"   æ¨¡å‹é™åˆ¶: {model_limit:,}")
                print(f"   å¯ç”¨ tokens: {available_tokens:,}")
                
                if attempt < max_retries:
                    # Try to truncate input
                    if isinstance(input_data, str):
                        truncated_input = token_manager.truncate_text_to_fit(
                            input_data, agent.model, reserved_tokens=2000
                        )
                        if truncated_input != input_data:
                            print(f"   ç¬¬ {attempt + 1} æ¬¡å˜—è©¦: æˆªæ–·è¼¸å…¥æ–‡æœ¬")
                            input_data = truncated_input
                            continue
                
                raise Exception(f"Token è¶…å‡ºé™åˆ¶: éœ€è¦ {estimated_input_tokens:,} tokensï¼Œä½†æ¨¡å‹ {agent.model} é™åˆ¶ç‚º {model_limit:,}")
            
            # Run the agent
            print(f"ğŸ¤– åŸ·è¡Œ {agent.name} - é ä¼° input tokens: {estimated_input_tokens:,}")
            
            if context:
                result = await Runner.run(agent, input_data, context=context)
            else:
                result = await Runner.run(agent, input_data)
            
            # Estimate output tokens and log usage
            output_text = str(result.final_output) if hasattr(result, 'final_output') else str(result)
            output_tokens = token_manager.count_tokens(output_text, agent.model)
            
            token_manager.log_usage(
                model=agent.model,
                input_tokens=estimated_input_tokens,
                output_tokens=output_tokens,
                agent_name=agent.name
            )
            
            return result
            
        except Exception as e:
            if attempt == max_retries:
                print(f"âŒ {agent.name} åŸ·è¡Œå¤±æ•— (å·²é‡è©¦ {max_retries} æ¬¡): {str(e)}")
                raise
            else:
                print(f"âš ï¸  {agent.name} ç¬¬ {attempt + 1} æ¬¡å˜—è©¦å¤±æ•—ï¼Œé‡è©¦ä¸­...")
                continue

# Token-aware streamed agent runner
async def run_agent_with_token_tracking_streamed(agent: Agent, 
                                               input_data: str, 
                                               context=None,
                                               max_retries: int = 2):
    """
    Run agent with token usage tracking and overflow protection for streamed responses
    
    Args:
        agent: The agent to run
        input_data: Input data for the agent
        context: Optional context
        max_retries: Maximum retries if token limit exceeded
        
    Returns:
        Streamed agent result with token tracking
    """
    for attempt in range(max_retries + 1):
        try:
            # Estimate input tokens
            estimated_input_tokens = token_manager.estimate_tokens_for_agent_input(
                instructions=agent.instructions,
                user_input=input_data,
                model=agent.model
            )
            
            # Check token limits
            is_within_limit, available_tokens, model_limit = token_manager.check_token_limit(
                estimated_input_tokens, agent.model
            )
            
            if not is_within_limit:
                print(f"âš ï¸  Token é™åˆ¶è­¦å‘Š - {agent.name} ({agent.model})")
                print(f"   é ä¼° input tokens: {estimated_input_tokens:,}")
                print(f"   æ¨¡å‹é™åˆ¶: {model_limit:,}")
                print(f"   å¯ç”¨ tokens: {available_tokens:,}")
                
                if attempt < max_retries:
                    # Try to truncate input
                    if isinstance(input_data, str):
                        truncated_input = token_manager.truncate_text_to_fit(
                            input_data, agent.model, reserved_tokens=2000
                        )
                        if truncated_input != input_data:
                            print(f"   ç¬¬ {attempt + 1} æ¬¡å˜—è©¦: æˆªæ–·è¼¸å…¥æ–‡æœ¬")
                            input_data = truncated_input
                            continue
                
                raise Exception(f"Token è¶…å‡ºé™åˆ¶: éœ€è¦ {estimated_input_tokens:,} tokensï¼Œä½†æ¨¡å‹ {agent.model} é™åˆ¶ç‚º {model_limit:,}")
            
            # Run the agent with streaming
            print(f"ğŸ¤– åŸ·è¡Œ {agent.name} (ä¸²æµæ¨¡å¼) - é ä¼° input tokens: {estimated_input_tokens:,}")
            
            if context:
                result = Runner.run_streamed(agent, input_data, context=context)
            else:
                result = Runner.run_streamed(agent, input_data)
            
            # Create a wrapper to track output tokens
            
            class TokenTrackingStreamResult:
                def __init__(self, original_result, agent, estimated_input_tokens):
                    self.original_result = original_result
                    self.agent = agent
                    self.estimated_input_tokens = estimated_input_tokens
                    self.output_content = ""
                    self.final_output = None
                
                async def stream_events(self):
                    async for event in self.original_result.stream_events():
                        # Track streaming content for token counting
                        if hasattr(event, 'data') and hasattr(event.data, 'delta'):
                            self.output_content += event.data.delta
                        yield event
                    
                    # After streaming is complete, log token usage
                    self.final_output = self.original_result.final_output
                    output_tokens = token_manager.count_tokens(str(self.final_output), self.agent.model)
                    
                    token_manager.log_usage(
                        model=self.agent.model,
                        input_tokens=self.estimated_input_tokens,
                        output_tokens=output_tokens,
                        agent_name=self.agent.name
                    )
                    
                    print(f"âœ… {self.agent.name} å®Œæˆ - input: {self.estimated_input_tokens:,} tokens, output: {output_tokens:,} tokens")
            
            return TokenTrackingStreamResult(result, agent, estimated_input_tokens)
            
        except Exception as e:
            if attempt == max_retries:
                print(f"âŒ {agent.name} åŸ·è¡Œå¤±æ•— (å·²é‡è©¦ {max_retries} æ¬¡): {str(e)}")
                raise
            else:
                print(f"âš ï¸  {agent.name} ç¬¬ {attempt + 1} æ¬¡å˜—è©¦å¤±æ•—ï¼Œé‡è©¦ä¸­...")
                continue

# Convenience functions for token management
def get_session_token_summary() -> str:
    """Get formatted token usage summary for current session"""
    return token_manager.get_session_summary()

def save_token_usage_log(filepath: str = "token_usage_log.json"):
    """Save token usage log to file"""
    token_manager.save_usage_log(filepath)
    print(f"ğŸ’¾ Token ä½¿ç”¨è¨˜éŒ„å·²ä¿å­˜è‡³: {filepath}")

def check_model_token_limits(model: str) -> Dict[str, int]:
    """Get token limit information for a specific model"""
    return {
        "model": model,
        "max_tokens": token_manager.get_model_limit(model),
        "encoding": token_manager.MODEL_ENCODINGS.get(model, "cl100k_base")
    }

def estimate_text_tokens(text: str, model: str = "gpt-4.1") -> int:
    """Estimate tokens for a given text and model"""
    return token_manager.count_tokens(text, model)


### ç·¨è¼¯æº–å‰‡æœå°‹Agent
async def edit_advice_process(media_name, input_text, news_map, yield_callback=None, edit_mode=None):
    print(f"==> ç”Ÿæˆç·¨è¼¯å»ºè­°...")
    start_time = time.time()

    vector_store_ids = get_editor_rules(media_name)

    edit_advice_agent = Agent(
        name="get_edit_advice",
        instructions=edit_advice_prompt,
        model="gpt-4.1",
        tools=[FileSearchTool(vector_store_ids=[vector_store_ids]), think],
        model_settings=ModelSettings(tool_choice="required")
    )

    user_input = f"ä½¿ç”¨FileSearchæª¢æŸ¥æ–°èæ–‡ç« æ˜¯å¦ç¬¦åˆç·¨è¼¯å®ˆå‰‡çš„å¯«ä½œè¦ç¯„ã€‚æ–°èæ–‡ç« ï¼š<article>{input_text}</article> è­¯åæª”ï¼š{news_map} ã€‚ä½¿ç”¨è€…æŒ‡å®šçš„ç·¨è¼¯æ¨¡å¼ï¼š{edit_mode}"
    
    # ä½¿ç”¨ä¸²æµæ¨¡å¼ä¾†è¿½è¹¤å·¥å…·èª¿ç”¨
    stream_result = await run_agent_with_token_tracking_streamed(edit_advice_agent, user_input)
    
    final_edit_advice = ""
    async for event in stream_result.stream_events():
        if event.type == "raw_response_event" and hasattr(event, 'data') and hasattr(event.data, 'type'):
            if event.data.type == "response.file_search_call.in_progress":
                print("ğŸ”§ å·¥å…·èª¿ç”¨: FileSearchTool")
            elif event.data.type == "response.function_call.start":
                print(f"ğŸ”§ å·¥å…·èª¿ç”¨: {event.data.name}")
            elif event.data.type == "response.output_text.delta":
                delta = getattr(event.data, 'delta', '')
                if delta:
                    final_edit_advice += delta
                    print(delta, end="", flush=True)
                    # å¦‚æœæœ‰å›èª¿å‡½æ•¸ï¼Œå°‡ delta å‚³éçµ¦å‰ç«¯
                    if yield_callback:
                        await yield_callback("edit_advice_delta", delta)

    end_time = time.time()
    print(f"\n>>> ç·¨è¼¯å»ºè­°è€—æ™‚ {end_time - start_time:.2f} ç§’")
    
    return final_edit_advice

async def edit_advice_process_streaming(media_name, input_text, news_map):
    """å°ˆç‚º streaming è¨­è¨ˆçš„ edit_advice_process generator ç‰ˆæœ¬"""
    print(f"==> ç”Ÿæˆç·¨è¼¯å»ºè­°...")
    start_time = time.time()

    vector_store_ids = get_editor_rules(media_name)

    edit_advice_agent = Agent(
        name="get_edit_advice",
        instructions=edit_advice_prompt,
        model="gpt-4.1",
        tools=[FileSearchTool(vector_store_ids=[vector_store_ids]), think],
        model_settings=ModelSettings(tool_choice="required")
    )

    user_input = f"ä½¿ç”¨FileSearchæª¢æŸ¥æ–°èæ–‡ç« æ˜¯å¦ç¬¦åˆç·¨è¼¯å®ˆå‰‡çš„å¯«ä½œè¦ç¯„ã€‚æ–°èæ–‡ç« ï¼š<article>{input_text}</article> è­¯åæª”ï¼š{news_map}"
    
    # ä½¿ç”¨ä¸²æµæ¨¡å¼ä¾†è¿½è¹¤å·¥å…·èª¿ç”¨
    stream_result = await run_agent_with_token_tracking_streamed(edit_advice_agent, user_input)
    
    final_edit_advice = ""
    async for event in stream_result.stream_events():
        if event.type == "raw_response_event" and hasattr(event, 'data') and hasattr(event.data, 'type'):
            if event.data.type == "response.file_search_call.in_progress":
                print("ğŸ”§ å·¥å…·èª¿ç”¨: FileSearchTool")
            elif event.data.type == "response.function_call.start":
                print(f"ğŸ”§ å·¥å…·èª¿ç”¨: {event.data.name}")
            elif event.data.type == "response.output_text.delta":
                delta = getattr(event.data, 'delta', '')
                if delta:
                    final_edit_advice += delta
                    print(delta, end="", flush=True)
                    # ç›´æ¥ yield streaming delta
                    yield ("streaming", delta)

    end_time = time.time()
    print(f"\n>>> ç·¨è¼¯å»ºè­°è€—æ™‚ {end_time - start_time:.2f} ç§’")
    
    # æœ€å¾Œ yield å®Œæˆçµæœ
    yield ("completed", final_edit_advice)

### èƒŒæ™¯æ‘˜è¦Agent
async def background_summary_process(input_text, media_name=None):

    print(f"==> ç”ŸæˆèƒŒæ™¯æ‘˜è¦...")
    start_time = time.time()

    class RelatedNews(BaseModel):
        title: str
        url: str

    class SummaryOutput(BaseModel):
        summary: str | None
        related_news: list[RelatedNews] | None

    # å‹•æ…‹ç”Ÿæˆ promptï¼ŒåŒ…å« media_name æŒ‡ç¤º
    dynamic_prompt = background_summary_prompt
    if media_name:
        dynamic_prompt += f"\n\né‡è¦ï¼šä½¿ç”¨find_related_newså·¥å…·æ™‚ï¼Œmedia_nameåƒæ•¸è«‹è¨­ç‚º: {media_name}"

    background_summary_agent = Agent(
        name="background_summary",
        instructions=dynamic_prompt,
        model="gpt-4.1",
        tools=[find_related_news],
        model_settings=ModelSettings(tool_choice="find_related_news"),
        output_type=SummaryOutput
    )

    result = await run_agent_with_token_tracking_streamed(background_summary_agent, input_text)

    async for event in result.stream_events():
        if event.type == "raw_response_event" and hasattr(event, 'data') and hasattr(event.data, 'type'):
            if event.data.type == "response.function_call.start":
                print(f"ğŸ”§ å·¥å…·èª¿ç”¨: {event.data.name}")

    end_time = time.time()
    print(f" >>> èƒŒæ™¯æ‘˜è¦è€—æ™‚ {end_time - start_time:.2f} ç§’")
    
    return result.final_output

### æ”¹ç¨¿Agent

# 1. åˆ¤æ–·æ”¹ç¨¿é¡å‹
async def classify_edit_mode(input_text: str, apponited_edit_mode: str = None) -> dict:
    """
    å–®ä¸€å‡½æ•¸å®Œæˆæ–°èé¡å‹åˆ†é¡
    """
    class news_type_output(BaseModel):
        edit_role: str  # "gov_roles", "foreign_roles", "normal_roles"
        edit_mode: str  # "gov_rewrite_rules", "foreign_rewrite_rules", "less_mode", "medium_mode", "full_mode"
        output_format: str  # "full_output_format", "normal_output_format"

    news_classifier = Agent(
        name="edit_mode_and_prompt",
        instructions="""
    <role>ä½ æ˜¯å°ç£çš„æ–°èç·¨è¼¯ï¼Œå¿«é€Ÿåˆ¤æ–·æ–°èé¡å‹ä¸¦ç”Ÿæˆæ”¹ç¨¿æŒ‡ä»¤</role>
    <task>
    1. èªè¨€åˆ¤æ–·ï¼š
        - åŒ…å«è‹±æ–‡ã€æ—¥æ–‡ã€å°å°¼æ–‡ç­‰å¤–èªå…§å®¹ â†’ edit_role="foreign_roles", edit_mode="foreign_rewrite_rules"
        - ç¹é«”ä¸­æ–‡å…§å®¹ â†’ ç¹¼çºŒä¸‹ä¸€æ­¥

    2. å…§å®¹é¡å‹åˆ¤æ–·ï¼ˆåªåœ¨ç„¡æ³•ç¢ºå®šæ™‚ä½¿ç”¨å·¥å…·ï¼‰ï¼š
        - æ˜é¡¯æ”¿åºœæ–°èç¨¿ç‰¹å¾µï¼ˆå®˜æ–¹ç”¨èªã€æ”¿ç­–å®£å°ã€éƒ¨æœƒç™¼è¨€ï¼‰ â†’ edit_role="gov_roles", edit_mode="gov_rewrite_rules"
        - ä¸€èˆ¬æ–°èå ±å° â†’ edit_role="normal_roles", edit_mode="less_mode"
        - ä¸ç¢ºå®šæ™‚æ‰ä½¿ç”¨ detect_gov_news å·¥å…·

    3. è¼¸å‡ºæ ¼å¼åˆ¤æ–·ï¼š
        - å¦‚æœ edit_role="gov_roles" or edit_role="foreign_roles"ï¼Œoutput_format="full_output_format"
        - å¦‚æœ edit_role="normal_roles"ï¼Œoutput_format="normal_output_format"
    </task>

    <final_output_format>
    {{
        "edit_role": "æ–°èç·¨è¼¯è§’è‰²",
        "edit_mode": "æ–°èç·¨è¼¯æ¨¡å¼: gov_rewrite_rules, foreign_rewrite_rules, less_mode, medium_mode, full_mode"
        "output_format": "æ–°èç·¨è¼¯æ ¼å¼: full_output_format, normal_output_format"
    }}
    </final_output_format>
        """,
        tools=[detect_gov_news],
        model="gpt-4.1",
        output_type=news_type_output
    )
    
    result = await run_agent_with_token_tracking(news_classifier, input_text)

    if apponited_edit_mode is not None:
        result.final_output.edit_mode = apponited_edit_mode
    
    classify_result =  {
        "edit_role": result.final_output.edit_role,
        "edit_mode": result.final_output.edit_mode,
        "output_format": result.final_output.output_format
    }
    return classify_result

# 2. æ”¹ç¨¿
async def rewrite_process(input_text, edit_advice, edit_mode, prompt, source_url_list=None):
    start_time = time.time()
    print("==> å•Ÿå‹•æ”¹ç¨¿...")

    # æ–‡ç« åˆ†æ®µè½
    elements, structure = text_split(input_text, edit_mode)
    paragraphs = [elem for elem in elements if elem != structure['separator']]

    # output format
    class Paragraph(BaseModel):
        original: str
        edited: str
        reason: str
        references_list: list[int]|None = None # åƒè€ƒè³‡æ–™ç·¨è™Ÿ

    class RewriteArticleOutput(BaseModel):
        rewrite_list: list[Paragraph]

    user_input = f"æ ¹æ“š<ç·¨è¼¯å»ºè­°>{edit_advice}</ç·¨è¼¯å»ºè­°>ï¼Œå°<åŸæ–‡>{paragraphs}</åŸæ–‡>é€²è¡Œæ”¹ç¨¿ã€‚"
    
    rewrite_agent = Agent(
        name="rewrite",
        instructions=prompt,
        model="gpt-4.1",
        output_type=RewriteArticleOutput,
    )
    
    result = await run_agent_with_token_tracking(rewrite_agent, user_input)
    
    # async for event in result.stream_events():
    #     if event.type == "raw_response_event" and hasattr(event, 'data') and hasattr(event.data, 'type'):
    #         if event.data.type == "response.function_call.start":
    #             print(f"ğŸ”§ å·¥å…·èª¿ç”¨: {event.data.name}")
    
    end_time = time.time()
    print(f" >>> æ”¹ç¨¿è€—æ™‚ {end_time - start_time:.2f} ç§’\n")

    # è½‰æ›ç‚ºèˆ‡ rewrite_workflow ç›¸åŒçš„è¼¸å‡ºæ ¼å¼
    rewrite_list = result.final_output.rewrite_list
    original_list = [paragraph.original for paragraph in rewrite_list]
    edited_list = [paragraph.edited for paragraph in rewrite_list]
    reason_list = [paragraph.reason for paragraph in rewrite_list]
    references_list = [paragraph.references_list for paragraph in rewrite_list if paragraph.references_list]

    final_output = {
        "Result": "Y",
        "ResultData": {
            "edit_type": edit_mode or "normal",
            "original_list": original_list,
            "edited_list": edited_list,
            "reason_list": reason_list,
            "references_list": references_list,
            "url_list": source_url_list or [],
            "elements": elements,
            "structure": structure 
        },
        "Message": "æ”¹ç¨¿å®Œæˆ"
    }

    return final_output

# if __name__ == "__main__":
#     load_dotenv()
#     api_key = os.getenv("OPENAI_API_KEY")
    
#     text = """ç¾åœ‹ç¸½çµ±å·æ™®ä»Šå¤©é‡å°ä¿„ç¾…æ–¯ç¸½çµ±è’²äº­è¡¨é”ã€Œæ¥µå…¶å¤±æœ›ã€ä¹‹æ„ï¼Œä¸¦è¡¨ç¤ºä»–çš„åŸ·æ”¿åœ˜éšŠè¨ˆåŠƒæ¡å–è¡Œå‹•é™ä½ä¿„çƒæˆ°çˆ­æ­»äº¡äººæ•¸ï¼Œä½†æœªèªªæ˜ç´°ç¯€ã€‚

# ç¶œåˆæ³•æ–°ç¤¾å’Œè·¯é€ç¤¾å ±å°ï¼Œè‡ª8æœˆåœ¨é˜¿æ‹‰æ–¯åŠ å·æœƒè¦‹è’²äº­ï¼ˆVladimir Putinï¼‰ä»¥ä¾†ï¼Œå·æ™®ï¼ˆDonald Trumpï¼‰æŒçºŒæ¨å‹•é€™ä½ä¿„åœ‹é ˜è¢–èˆ‡çƒå…‹è˜­ç¸½çµ±æ¾¤å€«æ–¯åŸºï¼ˆVolodymyr Zelenskyyï¼‰èˆ‰è¡Œé›™é‚Šæœƒè«‡ã€‚ç„¶è€Œï¼Œä¿„æ–¹åè€Œå»åŠ å¼·æ”»æ“Šçƒå…‹è˜­çš„åŠ›é“ã€‚

# å·æ™®åœ¨å»£æ’­ç¯€ç›®è¢«å•åŠæ˜¯å¦æœ‰é­è’²äº­èƒŒå›çš„æ„Ÿè¦ºï¼Œå‘Šè¨´ä¿å®ˆæ´¾è©•è«–å®¶è©¹å¯§æ–¯ï¼ˆScott Jenningsï¼‰èªªï¼šã€Œæˆ‘å°ç¸½çµ±è’²ä¸æ¥µå…¶å¤±æœ›ï¼Œæˆ‘å¯ä»¥é€™éº¼èªªã€‚ã€ä»–èªªï¼šã€Œæˆ‘å€‘æœ‰å¾ˆå¥½çš„äº¤æƒ…ï¼Œæˆ‘éå¸¸å¤±æœ›ã€‚ã€

# ç„¶è€Œï¼Œå·æ™®ä¸¦æ²’æœ‰èªªæ˜ä¿„ç¾…æ–¯æ˜¯å¦æœƒé¢è‡¨ä»»ä½•å¾Œæœï¼Œå³ä¾¿ä»–ä¸ä¹…å‰è¦æ±‚ä¿„çƒæ–¼å…©æ˜ŸæœŸå…§é”æˆå’Œå¹³å”è­°ï¼Œè€Œé€™å€‹æœŸé™å³å°‡æ–¼æœ¬é€±ç¨æ™šåˆ°æœŸã€‚

# ä»–è¡¨ç¤ºè‡ªå·±æœƒã€Œåšäº›ä»€éº¼ä¾†å¹«åŠ©æ°‘çœ¾ç”Ÿå­˜ä¸‹å»ã€ï¼Œä½†æ²’æœ‰å…·é«”èªªæ˜ç´°ç¯€ã€‚

# ä¸ä¹…å¾Œåœ¨æ©¢åœ“å½¢è¾¦å…¬å®¤æ™‚ï¼Œå•åŠä»–è¿‘æœŸæ˜¯å¦æ›¾èˆ‡è’²äº­é€šéè©±ï¼Œå·æ™®å›ç­”èªªï¼šã€Œæˆ‘å¾—çŸ¥æŸäº›éå¸¸æœ‰æ„æ€çš„äº‹ï¼Œæˆ‘æƒ³è«¸ä½æœªä¾†å¹¾å¤©å°±æœƒçŸ¥æ›‰äº†ã€‚ã€

# ä»–è£œå……èªªï¼Œå¦‚æœè’²äº­å’Œæ¾¤å€«æ–¯åŸºæœªèƒ½æœƒé¢ã€çµ‚çµè‡ª2022å¹´é–‹æ‰“è¿„ä»Šçš„ä¿„çƒæˆ°çˆ­ï¼Œå°‡æœƒå¼•ç™¼ã€Œå¾Œæœã€ã€‚

# å¦ä¸€æ–¹é¢ï¼Œè‡³æ–¼è’²äº­æ˜¨å¤©åœ¨åŒ—äº¬æœƒè¦‹ä¸­åœ‹åœ‹å®¶ä¸»å¸­ç¿’è¿‘å¹³ï¼Œä¸¦å°‡å‡ºå¸­å¤§å‹é–±å…µå¼ä¸€äº‹ï¼Œå·æ™®è¡¨ç¤ºå°ä¿„ä¸­å¯èƒ½å½¢æˆçš„è¯ç›Ÿä¸¦ä¸æ“”å¿ƒã€‚

# è¢«å•åŠæ˜¯å¦æ“”å¿ƒä¸­ä¿„çµç›Ÿå°æŠ—ç¾åœ‹ï¼Ÿå·æ™®å›ç­”ã€Œä¸€é»éƒ½ä¸æ“”å¿ƒã€ï¼Œæ¥è‘—è¡¨ç¤ºã€Œæˆ‘å€‘æœ‰å…¨çƒæœ€å¼·å¤§çš„è»éšŠï¼Œè€Œä¸”å¼·å¾ˆå¤šã€‚ä»–å€‘çµ•å°ä¸æœƒå°æˆ‘å€‘å‹•æ­¦ã€‚ç›¸ä¿¡æˆ‘ã€‚ã€ï¼ˆç·¨è­¯ï¼šè”¡ä½³æ•ï¼‰1140903"""
#     media_name = "CNA-medialab"

#     # è­¯åæª”æª¢æŸ¥
#     news_map, converted_text = process_translated_name(text, api_key)

#     # æœå°‹ç·¨è¼¯å®ˆå‰‡ -> ç·¨è¼¯å»ºè­°
#     edit_advice = asyncio.run(edit_advice_process(media_name, converted_text, news_map))

#     # äº‹ä»¶èƒŒæ™¯æ‘˜è¦
#     background_summary = asyncio.run(background_summary_process(text, media_name))
    
#     print(background_summary.summary)
    
#     source_url = []
#     if background_summary.related_news:
#         for i, news in enumerate(background_summary.related_news, 1):
#             print(f"{i}. {news.title}")
#             print(f"   ğŸ”— {news.url}\n")
#             source_url.append(news.url)
#     else:
#         source_url.append(None)
#         print("ç„¡ç›¸é—œæ–°è")

#     # åˆ¤æ–·æ–°èé¡å‹
#     news_type_result = asyncio.run(classify_edit_mode(converted_text, user_insert_mode="less_mode"))
#     print(news_type_result)

#     rewrite_prompt = generate_rewrite_prompt(news_type_result['edit_role'], news_type_result['edit_mode'], news_type_result['output_format'], background_summary.summary)

#     rewrite_result = asyncio.run(rewrite_process(converted_text, edit_advice, news_type_result['edit_mode'], rewrite_prompt, source_url))
    
#     # ç”Ÿæˆçµ¦å‰ç«¯çš„æ ¼å¼åŒ–çµæœ
#     if source_url:
#         formatted_result = formatted_output(rewrite_result, source_url, news_type_result['edit_mode'])
#         print(formatted_result)
